var documenterSearchIndex = {"docs":
[{"location":"#MPISchurComplements.jl","page":"MPISchurComplements.jl","title":"MPISchurComplements.jl","text":"","category":"section"},{"location":"#MPISchurComplements.MPISchurComplements","page":"MPISchurComplements.jl","title":"MPISchurComplements.MPISchurComplements","text":"Use the Schur complement technique to solve a block-2x2 matrix system, parallelised with MPI.\n\nSolve the matrix system\n\nleft(beginarraycc\nA  B\nC  D\nendarrayright)\ncdotleft(beginarrayc\nx\ny\nendarrayright)\n=left(beginarrayc\nu\nv\nendarrayright)\n\n\n\n\n\n","category":"module"},{"location":"#LinearAlgebra.ldiv!-Tuple{AbstractVector, AbstractVector, MPISchurComplement, AbstractVector, AbstractVector}","page":"MPISchurComplements.jl","title":"LinearAlgebra.ldiv!","text":"ldiv!(x::AbstractVector, y::AbstractVector, sc::MPISchurComplement,\n      u::AbstractVector, v::AbstractVector)\n\nSolve the 2x2 block-structured matrix system\n\nleft(beginarraycc\nA  B\nC  D\nendarrayright)\ncdot\nleft(beginarrayc\nx\ny\nendarrayright)\n=\nleft(beginarrayc\nu\nv\nendarrayright)\n\nfor x and v.\n\nOnly the locally owned parts of u and v should be passed, and the locally owned parts of the solution will be written into x and y. This means that x and u should correspond to the global indices in sc.owned_top_vector_entries and y and v should correspond to the global indices in sc.owned_bottom_vector_entries. When shared memory is used, x and y must be shared-memory arrays (identical on every process in sc.shared_comm). u and v should also be shared memory arrays (although non-shared identical copies would also work).\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.find_local_vector_inds-Tuple{AbstractArray, Any}","page":"MPISchurComplements.jl","title":"MPISchurComplements.find_local_vector_inds","text":"find_local_vector_inds(global_inds, owned_global_inds)\n\nFind indices of global_inds within owned_global_inds. This gives the 'local indices' corresponding to global_inds.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.mpi_schur_complement-Tuple{Any, AbstractMatrix, AbstractMatrix, AbstractMatrix, Union{UnitRange{Int64}, Vector{Int64}}, Union{UnitRange{Int64}, Vector{Int64}}}","page":"MPISchurComplements.jl","title":"MPISchurComplements.mpi_schur_complement","text":"mpi_schur_complement(A_factorization, B::AbstractMatrix, C::AbstractMatrix,\n                     D::AbstractMatrix,\n                     owned_top_vector_entries::Union{UnitRange{Int64},Vector{Int64}},\n                     owned_bottom_vector_entries::Union{UnitRange{Int64},Vector{Int64}};\n                     B_global_column_range::Union{UnitRange{Int64},Vector{Int64},Nothing}=nothing,\n                     C_global_row_range::Union{UnitRange{Int64},Vector{Int64},Nothing}=nothing,\n                     D_global_column_range::Union{UnitRange{Int64},Vector{Int64},Nothing}=nothing,\n                     distributed_comm::MPI.Comm=MPI.COMM_SELF,\n                     shared_comm::Union{MPI.Comm,Nothing}=nothing,\n                     allocate_array::Union{Function,Nothing}=nothing,\n                     synchronize_shared::Union{Function,Nothing}=nothing,\n                     use_sparse=true, separate_Ainv_B=false,\n                     parallel_schur=shared_comm!==nothing,\n                     schur_tile_size=nothing, skip_factorization=false,\n                     check_lu::Bool=true,\n                     timer::Union{TimerOutput,Nothing}=nothing)\n\nInitialise an MPISchurComplement struct representing a 2x2 block-structured matrix\n\nleft(beginarraycc\nA  B\nC  D\nendarrayright)\n\nA_factorization should be a matrix factorization of A, or an object with similar functionality, that can be passed to the ldiv!() function to solve a matrix system.\n\nB and D are only used to initialize the MPISchurComplement, they are not stored. If sparse=false is passed, a reference to C is stored. Only the locally owned parts of B, C and D should be passed. B, C and D may be modified by this function.\n\nowned_top_vector_entries gives the range of global indices that are owned by this process in the top block of the state vector, owned_bottom_vector_entries the same for the bottom block. The index ranges in owned_top_vector_entries and owned_bottom_vector_entries may have gaps, not being a contiguous range starting from 1 (and similarly for B_global_column_range, C_global_row_range and D_global_column_range, if they are passed, although they should all contain the same set of indices, across all processes, as owned_bottom_vector_entries). This may be useful if the 'bottom block' is actually, for example, a chunk from the middle of a grid, which separates the 'top block' into block-diagonal pieces. The global indexing for the full state vector can be used for both owned_top_vector_entries and owned_bottom_vector_entries, and they will be converted (by shifting indices down to remove the gaps) into a representation internal to the MPISchurComplement object that is contiguous and starts at 1 for each block. In addition, the indices passed do not have to be unique (they can be repeated on different subdomains, and/or within a single subdomain) or monotonically increasing - this allows for periodic dimensions where the final point in the dimension should be identified with the first point. When there are repeated indices, the vector entries passed for the right-hand-side (u and v) are assumed to be identical at the locations given corresponding to repeated indices. Matrix entries in B, C, and D corresponding to each repeated row or column index will be (in effect) summed together into a single entry of an 'assembled' matrix (as an implementation detail, the 'assembled' matrix may not be constructed explicitly, but it serves to define the meaning of repeated matrix entries).\n\nB should contain only the rows corresponding to owned_top_vector_entries and by default the columns corresponding to owned_bottom_vector_entries. If a different range of columns should be included, the corresponding global indices can be passed as B_global_column_range.\n\nC should contain only the columns corresponding to owned_top_vector_entries and by default the rows corresponding to owned_bottom_vector_entries. If a different range of rows should be included, the corresponding global indices can be passed as C_global_row_range.\n\nD should contain only the rows corresponding to owned_bottom_vector_entries and by default the same columns. If a different range of columns should be included, the corresponding global indices passed as D_global_column_range.\n\nWhen shared-memory MPI parallelism is used, B, C, and D should all be shared memory arrays which are identical on all processes in shared_comm (or non-shared identical copies, but that would be an inefficient use of memory). When owned_top_vector_entries and/or owned_bottom_vector_entries are ranges that overlap between different distributed-MPI ranks, the matrices A, B, C, and D should be set up so that adding together the matrices passed on each distributed-MPI rank gives the full, global matrix (i.e. there should be no double-counting of overlapping entries, but any fraction of the overlap can be passed on any distributed rank, as long as the contributions from each distributed rank add up to the full value).\n\ndistributed_comm and shared_comm are the MPI communicators to use for distributed-memory and shared-memory communications.\n\nallocate_shared_float and allocate_shared_int must be passed when shared_comm is passed. They should be passed functions that will be used to allocate various buffer arrays. allocate_shared_float should return arrays with the same element type as A_factorization, B, C, and D, while allocate_shared_int should return arrays with element type Int64. This is necessary as the MPI shared-memory 'windows' (MPI.Win) have to be stored and (eventually) freed. The 'windows' must be managed externally, as if they are freed by garbage collection, the freeing is not synchronized between different processes, causing errors.\n\nsynchronize_shared can be passed a function that synchronizes all processes in shared_comm. This may be useful if a custom synchronization is required for debugging, etc. If it is not passed, MPI.Barrier(shared_comm) will be used.\n\nBy default, makes C a sparse matrix. Pass use_sparse=false to force C to be dense.\n\nIf B is very sparse, and A_factorization is efficiently parallelised, it might be more efficient to compute A \\ (B.y) in two steps (B.y then A \\ ()) rather than multiplying by the dense Ainv_dot_B. If separate_Ainv_B=true is passed, this will be done (requires use_sparse=true). There is no saving in setup time because Ainv_dot_B still has to be calculated, to be multiplied by C when calculating schur_complement. There is also no memory saving as a dense B-sized buffer array is needed.\n\nBy default, when shared_comm is passed the DenseLUs module provided as part of MPISchurComplements.jl is used to factorize/solve the Schur complement matrix (which is dense) using a shared-memory parallel implementation (at present only the solve (ldiv!()) phase is parallelised, not the factorization (lu!)) and when shared_comm is not passed the serial implementation from LinearAlgebra (which uses LAPACK/BLAS) is used.  parallel_schur can be passed to force use of the serial (false) or parallel DenseLUs (true) implementations. When using DenseLUs, schur_tile_size can be used to set the tile_size argument to dense_lu(); the default is to use the smaller of 256 and the largest 2^n smaller than half of the size of the global 'bottom vector'.\n\nskip_factorization=true can be passed to create an MPISchurComplement instance without calculating the factorization corresponding to the input matrices. ldiv!() called with this instance will give incorrect results unless update_schur_complement!() is called first.\n\ncheck_lu=false can be passed to disable checks when performing dense LU factorizations. This may increase the speed of the factorization, but leaves it up to the user to guarantee correctness of the input matrices.\n\nA TimerOutput instance can be passed to timer to record timings of various subroutines.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.remove_gaps_in_ranges!-Tuple{Vector{Vector{Int64}}}","page":"MPISchurComplements.jl","title":"MPISchurComplements.remove_gaps_in_ranges!","text":"remove_gaps_in_ranges!(distributed_ranges::Vector{Vector{Int64}})\n\nWhere distributed_ranges does not include only a contiguous range (possibly overlapping) of indices starting from 1, identify and remove any 'gaps' (indices 1 or greater and less than the largest index in distributed_ranges which are not present in any element of distributed_ranges) from the index ranges.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.separate_repeated_indices-Tuple{Any}","page":"MPISchurComplements.jl","title":"MPISchurComplements.separate_repeated_indices","text":"separate_repeated_indices(inds)\n\nFind any repeated indices in inds. The first occurence of and index is the 'real' entry, and any subsequent occurences are 'repeats. Returns:\n\na Vector of 'real' entries\na Vector of the positions within the vector (i.e. the local indices) of the 'real' entries\na 2xn Matrix containing all the repeats, where the two entries in each column are the 'real' index and a corresponding 'repeat'\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.update_schur_complement!-Tuple{MPISchurComplement, Any, AbstractMatrix, AbstractMatrix, AbstractMatrix}","page":"MPISchurComplements.jl","title":"MPISchurComplements.update_schur_complement!","text":"update_schur_complement!(sc::MPISchurComplement, A, B::AbstractMatrix,\n                         C::AbstractMatrix, D::AbstractMatrix)\n\nUpdate the matrix which is being solved by sc.\n\nA will be passed to lu!(sc.A_factorization, A), so should be as required by the LU implementation being used for sc.A_factorization.\n\nB, C, and D should be the same shapes, and represent the same global index ranges, as the inputs to mpi_schur_complement() used to construct sc. B, C, and D may be modified.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.update_sparse_matrix!-Union{Tuple{Ti}, Tuple{Tf}, Tuple{SparseArrays.SparseMatrixCSC{Tf, Ti}, SparseArrays.SparseMatrixCSC{Tf, Ti}}} where {Tf, Ti}","page":"MPISchurComplements.jl","title":"MPISchurComplements.update_sparse_matrix!","text":"update_sparse_matrix!(A::SparseMatrixCSC{Tf,Ti},\n                      new_A::SparseMatrixCSC{Tf,Ti}) where {Tf,Ti}\n\nUpdate the values of A in-place to the values of new_A. May not be ideally efficient because it requires resizing Vectors.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.FakeMPILUs","page":"MPISchurComplements.jl","title":"MPISchurComplements.FakeMPILUs","text":"Inefficient distributed-MPI LU factorization, for testing.\n\nRequires local rows of matrix to be passed on each process. Just gathers the matrix and RHS vector to the root process, solves there, and scatters back.\n\nMatrix entries can be repeated (e.g. due to overlaps between different subdomains, or periodic boundary conditions). Repeated entries are added together into a single 'assembled' entry. The corresponding vector positions (in RHS and solution vectors) should also be repeated. The repeated entries in the RHS are ignored. Repeated entries in the solution vector are filled by copying the first appearance.\n\n\n\n\n\n","category":"module"}]
}
