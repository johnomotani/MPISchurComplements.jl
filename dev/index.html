<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MPISchurComplements.jl · MPISchurComplements</title><meta name="title" content="MPISchurComplements.jl · MPISchurComplements"/><meta property="og:title" content="MPISchurComplements.jl · MPISchurComplements"/><meta property="twitter:title" content="MPISchurComplements.jl · MPISchurComplements"/><meta name="description" content="Documentation for MPISchurComplements."/><meta property="og:description" content="Documentation for MPISchurComplements."/><meta property="twitter:description" content="Documentation for MPISchurComplements."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>MPISchurComplements</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>MPISchurComplements.jl</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>MPISchurComplements.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MPISchurComplements.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/johnomotani/MPISchurComplements.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="MPISchurComplements.jl"><a class="docs-heading-anchor" href="#MPISchurComplements.jl">MPISchurComplements.jl</a><a id="MPISchurComplements.jl-1"></a><a class="docs-heading-anchor-permalink" href="#MPISchurComplements.jl" title="Permalink"></a></h1><article><details class="docstring" open="true"><summary id="MPISchurComplements.MPISchurComplements"><a class="docstring-binding" href="#MPISchurComplements.MPISchurComplements"><code>MPISchurComplements.MPISchurComplements</code></a> — <span class="docstring-category">Module</span></summary><section><div><p>Use the Schur complement technique to solve a block-2x2 matrix system, parallelised with MPI.</p><p>Solve the matrix system</p><p class="math-container">\[\left(\begin{array}{cc}
A &amp; B\\
C &amp; D
\end{array}\right)
\cdot\left(\begin{array}{c}
x\\
y
\end{array}\right)
=\left(\begin{array}{c}
u\\
v
\end{array}\right)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L1-L20">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebra.ldiv!-Tuple{AbstractVector, AbstractVector, MPISchurComplement, AbstractVector, AbstractVector}"><a class="docstring-binding" href="#LinearAlgebra.ldiv!-Tuple{AbstractVector, AbstractVector, MPISchurComplement, AbstractVector, AbstractVector}"><code>LinearAlgebra.ldiv!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">ldiv!(x::AbstractVector, y::AbstractVector, sc::MPISchurComplement,
      u::AbstractVector, v::AbstractVector)</code></pre><p>Solve the 2x2 block-structured matrix system</p><p class="math-container">\[\left(\begin{array}{cc}
A &amp; B\\
C &amp; D
\end{array}\right)
\cdot
\left(\begin{array}{c}
x\\
y
\end{array}\right)
=
\left(\begin{array}{c}
u\\
v
\end{array}\right)\]</p><p>for <span>$x$</span> and <span>$v$</span>.</p><p>Only the locally owned parts of <code>u</code> and <code>v</code> should be passed, and the locally owned parts of the solution will be written into <code>x</code> and <code>y</code>. This means that <code>x</code> and <code>u</code> should correspond to the global indices in <code>sc.owned_top_vector_entries</code> and <code>y</code> and <code>v</code> should correspond to the global indices in <code>sc.owned_bottom_vector_entries</code>. When shared memory is used, <code>x</code> and <code>y</code> must be shared-memory arrays (identical on every process in <code>sc.shared_comm</code>). <code>u</code> and <code>v</code> should also be shared memory arrays (although non-shared identical copies would also work).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L1162-L1192">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.find_local_vector_inds-Tuple{AbstractArray, Any}"><a class="docstring-binding" href="#MPISchurComplements.find_local_vector_inds-Tuple{AbstractArray, Any}"><code>MPISchurComplements.find_local_vector_inds</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">find_local_vector_inds(global_inds, owned_global_inds)</code></pre><p>Find indices of <code>global_inds</code> within <code>owned_global_inds</code>. This gives the &#39;local indices&#39; corresponding to <code>global_inds</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L227-L232">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.mpi_schur_complement-Tuple{Any, AbstractMatrix, AbstractMatrix, AbstractMatrix, Union{UnitRange{Int64}, Vector{Int64}}, Union{UnitRange{Int64}, Vector{Int64}}}"><a class="docstring-binding" href="#MPISchurComplements.mpi_schur_complement-Tuple{Any, AbstractMatrix, AbstractMatrix, AbstractMatrix, Union{UnitRange{Int64}, Vector{Int64}}, Union{UnitRange{Int64}, Vector{Int64}}}"><code>MPISchurComplements.mpi_schur_complement</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">mpi_schur_complement(A_factorization, B::AbstractMatrix, C::AbstractMatrix,
                     D::AbstractMatrix,
                     owned_top_vector_entries::Union{UnitRange{Int64},Vector{Int64}},
                     owned_bottom_vector_entries::Union{UnitRange{Int64},Vector{Int64}};
                     B_global_column_range::Union{UnitRange{Int64},Vector{Int64},Nothing}=nothing,
                     C_global_row_range::Union{UnitRange{Int64},Vector{Int64},Nothing}=nothing,
                     D_global_column_range::Union{UnitRange{Int64},Vector{Int64},Nothing}=nothing,
                     distributed_comm::MPI.Comm=MPI.COMM_SELF,
                     shared_comm::Union{MPI.Comm,Nothing}=nothing,
                     allocate_array::Union{Function,Nothing}=nothing,
                     synchronize_shared::Union{Function,Nothing}=nothing,
                     use_sparse=true, separate_Ainv_B=false,
                     parallel_schur=shared_comm!==nothing,
                     schur_tile_size=nothing, skip_factorization=false,
                     check_lu::Bool=true,
                     timer::Union{TimerOutput,Nothing}=nothing)</code></pre><p>Initialise an MPISchurComplement struct representing a 2x2 block-structured matrix</p><p class="math-container">\[\left(\begin{array}{cc}
A &amp; B\\
C &amp; D
\end{array}\right)\]</p><p><code>A_factorization</code> should be a matrix factorization of <code>A</code>, or an object with similar functionality, that can be passed to the <code>ldiv!()</code> function to solve a matrix system.</p><p><code>B</code> and <code>D</code> are only used to initialize the MPISchurComplement, they are not stored. If <code>sparse=false</code> is passed, a reference to <code>C</code> is stored. Only the locally owned parts of <code>B</code>, <code>C</code> and <code>D</code> should be passed. <code>B</code>, <code>C</code> and <code>D</code> may be modified by this function.</p><p><code>owned_top_vector_entries</code> gives the range of global indices that are owned by this process in the top block of the state vector, <code>owned_bottom_vector_entries</code> the same for the bottom block. The index ranges in <code>owned_top_vector_entries</code> and <code>owned_bottom_vector_entries</code> may have gaps, not being a contiguous range starting from 1 (and similarly for <code>B_global_column_range</code>, <code>C_global_row_range</code> and <code>D_global_column_range</code>, if they are passed, although they should all contain the same set of indices, across all processes, as <code>owned_bottom_vector_entries</code>). This may be useful if the &#39;bottom block&#39; is actually, for example, a chunk from the middle of a grid, which separates the &#39;top block&#39; into block-diagonal pieces. The global indexing for the full state vector can be used for both <code>owned_top_vector_entries</code> and <code>owned_bottom_vector_entries</code>, and they will be converted (by shifting indices down to remove the gaps) into a representation internal to the <code>MPISchurComplement</code> object that is contiguous and starts at 1 for each block. In addition, the indices passed do not have to be unique (they can be repeated on different subdomains, and/or within a single subdomain) or monotonically increasing - this allows for periodic dimensions where the final point in the dimension should be identified with the first point. When there are repeated indices, the vector entries passed for the right-hand-side (<code>u</code> and <code>v</code>) are assumed to be identical at the locations given corresponding to repeated indices. Matrix entries in <code>B</code>, <code>C</code>, and <code>D</code> corresponding to each repeated row or column index will be (in effect) summed together into a single entry of an &#39;assembled&#39; matrix (as an implementation detail, the &#39;assembled&#39; matrix may not be constructed explicitly, but it serves to define the meaning of repeated matrix entries).</p><p><code>B</code> should contain only the rows corresponding to <code>owned_top_vector_entries</code> and by default the columns corresponding to <code>owned_bottom_vector_entries</code>. If a different range of columns should be included, the corresponding global indices can be passed as <code>B_global_column_range</code>.</p><p><code>C</code> should contain only the columns corresponding to <code>owned_top_vector_entries</code> and by default the rows corresponding to <code>owned_bottom_vector_entries</code>. If a different range of rows should be included, the corresponding global indices can be passed as <code>C_global_row_range</code>.</p><p><code>D</code> should contain only the rows corresponding to <code>owned_bottom_vector_entries</code> and by default the same columns. If a different range of columns should be included, the corresponding global indices passed as <code>D_global_column_range</code>.</p><p>When shared-memory MPI parallelism is used, <code>B</code>, <code>C</code>, and <code>D</code> should all be shared memory arrays which are identical on all processes in <code>shared_comm</code> (or non-shared identical copies, but that would be an inefficient use of memory). When <code>owned_top_vector_entries</code> and/or <code>owned_bottom_vector_entries</code> are ranges that overlap between different distributed-MPI ranks, the matrices <code>A</code>, <code>B</code>, <code>C</code>, and <code>D</code> should be set up so that adding together the matrices passed on each distributed-MPI rank gives the full, global matrix (i.e. there should be no double-counting of overlapping entries, but any fraction of the overlap can be passed on any distributed rank, as long as the contributions from each distributed rank add up to the full value).</p><p><code>distributed_comm</code> and <code>shared_comm</code> are the MPI communicators to use for distributed-memory and shared-memory communications.</p><p><code>allocate_shared_float</code> and <code>allocate_shared_int</code> must be passed when <code>shared_comm</code> is passed. They should be passed functions that will be used to allocate various buffer arrays. <code>allocate_shared_float</code> should return arrays with the same element type as <code>A_factorization</code>, <code>B</code>, <code>C</code>, and <code>D</code>, while <code>allocate_shared_int</code> should return arrays with element type <code>Int64</code>. This is necessary as the MPI shared-memory &#39;windows&#39; (<code>MPI.Win</code>) have to be stored and (eventually) freed. The &#39;windows&#39; must be managed externally, as if they are freed by garbage collection, the freeing is not synchronized between different processes, causing errors.</p><p><code>synchronize_shared</code> can be passed a function that synchronizes all processes in <code>shared_comm</code>. This may be useful if a custom synchronization is required for debugging, etc. If it is not passed, <code>MPI.Barrier(shared_comm)</code> will be used.</p><p>By default, makes <code>C</code> a sparse matrix. Pass <code>use_sparse=false</code> to force <code>C</code> to be dense.</p><p>If <code>B</code> is very sparse, and <code>A_factorization</code> is efficiently parallelised, it might be more efficient to compute <code>A \ (B.y)</code> in two steps (<code>B.y</code> then <code>A \ ()</code>) rather than multiplying by the dense <code>Ainv_dot_B</code>. If <code>separate_Ainv_B=true</code> is passed, this will be done (requires <code>use_sparse=true</code>). There is no saving in setup time because <code>Ainv_dot_B</code> still has to be calculated, to be multiplied by <code>C</code> when calculating <code>schur_complement</code>. There is also no memory saving as a dense B-sized buffer array is needed.</p><p>By default, when <code>shared_comm</code> is passed the DenseLUs module provided as part of MPISchurComplements.jl is used to factorize/solve the Schur complement matrix (which is dense) using a shared-memory parallel implementation (at present only the solve (<code>ldiv!()</code>) phase is parallelised, not the factorization (<code>lu!</code>)) and when <code>shared_comm</code> is not passed the serial implementation from LinearAlgebra (which uses LAPACK/BLAS) is used.  <code>parallel_schur</code> can be passed to force use of the serial (<code>false</code>) or parallel DenseLUs (<code>true</code>) implementations. When using DenseLUs, <code>schur_tile_size</code> can be used to set the <code>tile_size</code> argument to <code>dense_lu()</code>; the default is to use the smaller of 256 and the largest 2^n smaller than half of the size of the global &#39;bottom vector&#39;.</p><p><code>skip_factorization=true</code> can be passed to create an MPISchurComplement instance without calculating the factorization corresponding to the input matrices. <code>ldiv!()</code> called with this instance will give incorrect results unless <code>update_schur_complement!()</code> is called first.</p><p><code>check_lu=false</code> can be passed to disable checks when performing dense LU factorizations. This may increase the speed of the factorization, but leaves it up to the user to guarantee correctness of the input matrices.</p><p>A <code>TimerOutput</code> instance can be passed to <code>timer</code> to record timings of various subroutines.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L267-L393">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.remove_gaps_in_ranges!-Tuple{Vector{Vector{Int64}}}"><a class="docstring-binding" href="#MPISchurComplements.remove_gaps_in_ranges!-Tuple{Vector{Vector{Int64}}}"><code>MPISchurComplements.remove_gaps_in_ranges!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">remove_gaps_in_ranges!(distributed_ranges::Vector{Vector{Int64}})</code></pre><p>Where <code>distributed_ranges</code> does not include only a contiguous range (possibly overlapping) of indices starting from 1, identify and remove any &#39;gaps&#39; (indices 1 or greater and less than the largest index in <code>distributed_ranges</code> which are not present in any element of <code>distributed_ranges</code>) from the index ranges.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L112-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.separate_repeated_indices-Tuple{Any}"><a class="docstring-binding" href="#MPISchurComplements.separate_repeated_indices-Tuple{Any}"><code>MPISchurComplements.separate_repeated_indices</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">separate_repeated_indices(inds)</code></pre><p>Find any repeated indices in <code>inds</code>. The first occurence of and index is the &#39;real&#39; entry, and any subsequent occurences are &#39;repeats. Returns:</p><ol><li>a Vector of &#39;real&#39; entries</li><li>a Vector of the positions within the vector (i.e. the local indices) of the &#39;real&#39; entries</li><li>a 2xn Matrix containing all the repeats, where the two entries in each column are the &#39;real&#39; index and a corresponding &#39;repeat&#39;</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L171-L180">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.update_schur_complement!-Tuple{MPISchurComplement, Any, AbstractMatrix, AbstractMatrix, AbstractMatrix}"><a class="docstring-binding" href="#MPISchurComplements.update_schur_complement!-Tuple{MPISchurComplement, Any, AbstractMatrix, AbstractMatrix, AbstractMatrix}"><code>MPISchurComplements.update_schur_complement!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">update_schur_complement!(sc::MPISchurComplement, A, B::AbstractMatrix,
                         C::AbstractMatrix, D::AbstractMatrix)</code></pre><p>Update the matrix which is being solved by <code>sc</code>.</p><p><code>A</code> will be passed to <code>lu!(sc.A_factorization, A)</code>, so should be as required by the LU implementation being used for <code>sc.A_factorization</code>.</p><p><code>B</code>, <code>C</code>, and <code>D</code> should be the same shapes, and represent the same global index ranges, as the inputs to <code>mpi_schur_complement()</code> used to construct <code>sc</code>. <code>B</code>, <code>C</code>, and <code>D</code> may be modified.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L923-L935">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.update_sparse_matrix!-Union{Tuple{Ti}, Tuple{Tf}, Tuple{SparseArrays.SparseMatrixCSC{Tf, Ti}, SparseArrays.SparseMatrixCSC{Tf, Ti}}} where {Tf, Ti}"><a class="docstring-binding" href="#MPISchurComplements.update_sparse_matrix!-Union{Tuple{Ti}, Tuple{Tf}, Tuple{SparseArrays.SparseMatrixCSC{Tf, Ti}, SparseArrays.SparseMatrixCSC{Tf, Ti}}} where {Tf, Ti}"><code>MPISchurComplements.update_sparse_matrix!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">update_sparse_matrix!(A::SparseMatrixCSC{Tf,Ti},
                      new_A::SparseMatrixCSC{Tf,Ti}) where {Tf,Ti}</code></pre><p>Update the values of <code>A</code> in-place to the values of <code>new_A</code>. May not be ideally efficient because it requires resizing Vectors.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/MPISchurComplements.jl#L243-L249">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MPISchurComplements.FakeMPILUs"><a class="docstring-binding" href="#MPISchurComplements.FakeMPILUs"><code>MPISchurComplements.FakeMPILUs</code></a> — <span class="docstring-category">Module</span></summary><section><div><p>Inefficient distributed-MPI LU factorization, for testing.</p><p>Requires local rows of matrix to be passed on each process. Just gathers the matrix and RHS vector to the root process, solves there, and scatters back.</p><p>Matrix entries can be repeated (e.g. due to overlaps between different subdomains, or periodic boundary conditions). Repeated entries are added together into a single &#39;assembled&#39; entry. The corresponding vector positions (in RHS and solution vectors) should also be repeated. The repeated entries in the RHS are ignored. Repeated entries in the solution vector are filled by copying the first appearance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/johnomotani/MPISchurComplements.jl/blob/7f563a6173c5542f9020f72324280f0de93b597c/src/FakeMPILUs.jl#L1-L12">source</a></section></details></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 17 February 2026 12:27">Tuesday 17 February 2026</span>. Using Julia version 1.11.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
