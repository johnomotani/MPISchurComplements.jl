var documenterSearchIndex = {"docs":
[{"location":"#MPISchurComplements.jl","page":"MPISchurComplements.jl","title":"MPISchurComplements.jl","text":"","category":"section"},{"location":"#MPISchurComplements.MPISchurComplements","page":"MPISchurComplements.jl","title":"MPISchurComplements.MPISchurComplements","text":"Use the Schur complement technique to solve a block-2x2 matrix system, parallelised with MPI.\n\nSolve the matrix system\n\nleft(beginarraycc\nA  B\nC  D\nendarrayright)\ncdotleft(beginarrayc\nx\ny\nendarrayright)\n=left(beginarrayc\nu\nv\nendarrayright)\n\n\n\n\n\n","category":"module"},{"location":"#LinearAlgebra.ldiv!-Tuple{AbstractVector, AbstractVector, MPISchurComplement, AbstractVector, AbstractVector}","page":"MPISchurComplements.jl","title":"LinearAlgebra.ldiv!","text":"ldiv!(x::AbstractVector, y::AbstractVector, sc::MPISchurComplement,\n      u::AbstractVector, v::AbstractVector)\n\nSolve the 2x2 block-structured matrix system\n\nleft(beginarraycc\nA  B\nC  D\nendarrayright)\ncdot\nleft(beginarrayc\nx\ny\nendarrayright)\n=\nleft(beginarrayc\nu\nv\nendarrayright)\n\nfor x and v.\n\nOnly the locally owned parts of u and v should be passed, and the locally owned parts of the solution will be written into x and y. This means that x and u should correspond to the global indices in sc.owned_top_vector_entries and y and v should correspond to the global indices in sc.owned_bottom_vector_entries. When shared memory is used, x and y must be shared-memory arrays (identical on every process in sc.shared_comm). u and v should also be shared memory arrays (although non-shared identical copies would also work).\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.get_neighbour_comms-Tuple{MPI.Comm}","page":"MPISchurComplements.jl","title":"MPISchurComplements.get_neighbour_comms","text":"Get communicators which include this rank and one neighbouring rank.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.mpi_schur_complement-Tuple{Any, AbstractMatrix, UnitRange{Int64}, AbstractMatrix, UnitRange{Int64}, AbstractMatrix, UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}","page":"MPISchurComplements.jl","title":"MPISchurComplements.mpi_schur_complement","text":"mpi_schur_complement(A_factorization, B::AbstractMatrix,\n                     B_global_column_range::UnitRange{Int64}, C::AbstractMatrix,\n                     C_global_row_range::UnitRange{Int64}, D::AbstractMatrix,\n                     D_global_column_range::UnitRange{Int64},\n                     owned_top_vector_entries::UnitRange{Int64},\n                     owned_bottom_vector_entries::UnitRange{Int64};\n                     distributed_comm::MPI.Comm=MPI.COMM_SELF,\n                     shared_comm::Union{MPI.Comm,Nothing}=nothing,\n                     allocate_array::Union{Function,Nothing}=nothing)\n\nInitialise an MPISchurComplement struct representing a 2x2 block-structured matrix\n\nleft(beginarraycc\nA  B\nC  D\nendarrayright)\n\nA_factorization should be a matrix factorization of A, or an object with similar functionality, that can be passed to the ldiv!() function to solve a matrix system.\n\nB and D are only used to initialize the MPISchurComplement, they are not stored. A reference to C is stored. Only the locally owned parts of B, C and D should be passed. B, C and D may be modified by this function. owned_top_vector_entries gives the range of global indices that are owned by this process in the top block of the state vector, owned_bottom_vector_entries the same for the bottom block. B should contain only the rows corresponding to owned_top_vector_entries and the columns corresponding to the global indices given by B_global_column_range (these do not have to be all the global indices, when it is known that the locally owned rows of B are zero outside of B_global_column_range). C should contain only the columns corresponding to owned_top_vector_entries and the rows corresponding to the global indices given by C_global_row_range (these do not have to be all the global indices, when it is known that the locally owned columns of C are zero outside of C_global_row_range). D should contain only the rows corresponding to owned_bottom_vector_entries and the columns corresponding to the global indices given by D_global_column_range (these do not have to be all the global indices, when it is known that the locally owned rows of D are zero outside of D_global_column_range). When shared-memory MPI parallelism is used, B, C, and D should all be shared memory arrays which are identical on all processes in shared_comm (or non-shared identical copies, but that would be an inefficient use of memory). When owned_top_vector_entries and/or owned_bottom_vector_entries are ranges that overlap between different distributed-MPI ranks, the matrices A, B, C, and D should be set up so that adding together the matrices passed on each distributed-MPI rank gives the full, global matrix (i.e. there should be no double-counting of overlapping entries, but any fraction of the overlap can be passed on any distributed rank, as long as the contributions from each distributed rank add up to the full value).\n\ndistributed_comm and shared_comm are the MPI communicators to use for distributed-memory and shared-memory communications.\n\nallocate_array is required when shared_comm is passed. It should be passed a function that will be used to allocate various buffer arrays. It should return arrays with the same element type as A_factorization, B, C, and D. This is necessary as the MPI shared-memory 'windows' (MPI.Win) have to be stored and (eventually) freed. The 'windows' must be managed externally, as if they are freed by garbage collection, the freeing is not synchronized between different processes, causing errors.\n\nsynchronize_shared can be passed a function that synchronizes all processes in shared_comm. This may be useful if a custom synchronization is required for debugging, etc. If it is not passed, MPI.Barrier(shared_comm) will be used.\n\n\n\n\n\n","category":"method"},{"location":"#MPISchurComplements.update_schur_complement!-Tuple{MPISchurComplement, Any, AbstractMatrix, AbstractMatrix, AbstractMatrix}","page":"MPISchurComplements.jl","title":"MPISchurComplements.update_schur_complement!","text":"update_schur_complement!(sc::MPISchurComplement, A, B::AbstractMatrix,\n                         C::AbstractMatrix, D::AbstractMatrix)\n\nUpdate the matrix which is being solved by sc.\n\nA will be passed to lu!(sc.A_factorization, A), so should be as required by the LU implementation being used for sc.A_factorization.\n\nB, C, and D should be the same shapes, and represent the same global index ranges, as the inputs to mpi_schur_complement() used to construct sc. B, C, and D may be modified.\n\n\n\n\n\n","category":"method"}]
}
